{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html dir='rtl'>\n",
    "<h2 dir='rtl'>\n",
    "توضیخ کلی کار\n",
    "</h2>\n",
    "<p dir='rtl'>\n",
    "در این نوت بوک،‌ مقاله‌هایی در زمینه مدل‌رانه، توصیه‌گر و استفاده از توصیه‌گر در فرایند مدل‌رانه مورد پردازش قرار گرفته است تا تراکم کلمات استفاده شده در آن و کلماتی که به یکدیگر مرتبط هستند، مشخص شوند.\n",
    "</p>\n",
    "\n",
    "<h2 dir='rtl'>\n",
    "نوت بوک به تفکیک بخش‌ها\n",
    "</h2>\n",
    "<p dir='rtl'>\n",
    "در ادامه بخش‌های مختلف نوت بوک به همراه توضیحاتی درمورد هر بخش، آورده شده است.\n",
    "</p>\n",
    "<h3 dir='rtl'>\n",
    "نصب پیش‌نیازها\n",
    "</h3>\n",
    "<p dir='rtl'>\n",
    "در اولین سلول این نوت بوک، پیش‌نیازهایی که برای اجرای این نوت بوک مورد نیاز است آورده شده است تا نصب شوند. بنابراین این سلول باید قبل از اجرای سلول‌های دیگر اجرا شود.\n",
    "</p>\n",
    "\n",
    "<h3 dir='rtl'>\n",
    "درون‌ریزی کتابخانه‌ها و تنظیمات اولیه\n",
    "</h3>\n",
    "<p dir='rtl'>\n",
    "در این بخش، کتابخانه‌هایی که در ادامه مورد نیاز بوده اند درون‌ریزی شده اند و برخی کتابخانه‌ها که نیاز به پیش‌تنظیمات داشته اند، تنظیم شده اند.\n",
    "</p>\n",
    "\n",
    "<h3 dir='rtl'>\n",
    "خزنده\n",
    "</h3>\n",
    "<p dir='rtl'>\n",
    "در این بخش، خزنده کلیه‌ی فایل‌های پی دی اف را پردازش کرده و متن آنها را از صفحات مختلف استخراح می‌کند تا در مراحل بعد پیش‌پردازش و پردازش روی متن استخراج شده انجام گیرد.\n",
    "</p>\n",
    "\n",
    "<h3 dir='rtl'>\n",
    "ابزار تمیزکردن متن و جملات\n",
    "</h3>\n",
    "<p dir='rtl'>\n",
    "در این بخش ابزارهایی به منظور بالابردن خوانایی در پیش‌پردازش تعریف شده اند که یکی برای تمیز کردن متنی است که به صورت خام استخراج شده است و دیگری برای تمیز کردن جملات.\n",
    "</p>\n",
    "\n",
    "<h3 dir='rtl'>\n",
    "خواندن دیتاست و پیش‌پردازش آن\n",
    "</h3>\n",
    "<p dir='rtl'>\n",
    "در این گام دیتاستی که ذخیره شده بود، خوانده می‌شود و سپس مراحل پیش‌پردازش به کمک ابزارهای تعریف‌شده در بخش قبل و ابزارهای کتابخانه‌های درون‌ریزی شده مانند nltk، انجام می‌شوند.\n",
    "</p>\n",
    "\n",
    "<h3 dir='rtl'>\n",
    "ابزار پردازش وردنت\n",
    "</h3>\n",
    "<p dir='rtl'>\n",
    "برای بهتر شدن خوانایی کد،‌ دو تابع تعریف شده اند که به ترتیب بسامد کلمات و ارتباط آنها با یکدیگر را توسط وردنت تحلیل می‌کند.\n",
    "</p>\n",
    "\n",
    "<h3 dir='rtl'>\n",
    "پردازش آماری\n",
    "</h3>\n",
    "<p dir='rtl'>\n",
    "در این بخش، آماری از کل جملات مقالات آورده شده است که از جمله‌ی آن تراکم کلمات در جملات و ارتباط بین کلمات شبیه به یکدیگر و گروه‌بندی آنها هستند.\n",
    "</p>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "\n",
    "%pip install pypdf2\n",
    "%pip install pycryptodome\n",
    "%pip install nltk\n",
    "%pip install tqdm\n",
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required modules and initial config\n",
    "\n",
    "import PyPDF2\n",
    "import glob\n",
    "import pickle\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from string import *\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "\n",
    "DATASEST_FILENAME = 'dataset.pickle'\n",
    "nlp = spacy.load('en_core_web_sm') # or whatever model you have installed\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    }
   ],
   "source": [
    "# Crawl all text of the articles\n",
    "\n",
    "all_text = []\n",
    "\n",
    "for f in glob.glob(\"corpus/*.pdf\"):\n",
    "    \n",
    "    with open(f, 'rb') as f_obj:\n",
    "        file_sentences = []\n",
    "        \n",
    "        # creating a pdf reader object\n",
    "        pdfReader = PyPDF2.PdfFileReader(f_obj)\n",
    "  \n",
    "        # printing number of pages in pdf file\n",
    "        for p in range(pdfReader.numPages):\n",
    "            # creating a page object\n",
    "            pageObj = pdfReader.getPage(p)\n",
    "        \n",
    "            # extracting text from page\n",
    "            file_sentences.append(pageObj.extractText())\n",
    "            \n",
    "        all_text.append(file_sentences)\n",
    "        \n",
    "with open(DATASEST_FILENAME, 'wb+') as f:\n",
    "    pickle.dump(all_text, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitizing text and sentences utils\n",
    "\n",
    "def sanitize_text(text):\n",
    "    # try to remove all refrencese \n",
    "    text = re.sub(r'\\n\\[\\s*\\d\\s*\\][^\\n]+\\n', r'\\n', text)\n",
    "    \n",
    "    # remove links\n",
    "    text = re.sub(r'http[\\S]+[\\s]', '', text)\n",
    "    text = re.sub(r'(www|WWW)[\\S]+[\\s]', '', text)\n",
    "    \n",
    "    # remove spacee before the dash in names having dash\n",
    "    text = re.sub(r'[ ](-\\w)', r'\\1', text)\n",
    "\n",
    "    # complete sentences splited by breakline\n",
    "    text = re.sub(r'-[\\n]', '', text)\n",
    "\n",
    "    # remove non-content setntences\n",
    "    text = re.sub(r'[\\n].+[^.][\\n]', '\\n', text)\n",
    "\n",
    "    # replace multispace with single space\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "\n",
    "    # remove references\n",
    "    text = re.sub(r'\\[.+?\\]', '', text)\n",
    "    \n",
    "    # remove three dots\n",
    "    text = re.sub(r'[.]{3} | [*]', '', text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def normalize_sentence(tokenized_sents, minimum_length=2, stopword_removal=True, stopwords_domain=[], lower_case=False, punctuation_removal=True):\n",
    "    '''\n",
    "    normalization function\n",
    "    '''\n",
    "    normalized_sents = tokenized_sents\n",
    "    \n",
    "    if stopword_removal:\n",
    "        # Remove stopwords in English and also the given domain stopwords\n",
    "        stopwords = [x.lower() for x in nltk.corpus.stopwords.words('english')]\n",
    "        normalized_sents=[[word for word in sentence if (word.lower() not in stopwords_domain + stopwords)] for sentence in tokenized_sents ]\n",
    "\n",
    "    if punctuation_removal:\n",
    "        # Remove punctuations\n",
    "        normalized_sents=[[word for word in sentence if word not in string.punctuation] for sentence in normalized_sents ]\n",
    "\n",
    "    if lower_case:\n",
    "        # Convert everything to lowercase and filter based on a min length\n",
    "        normalized_sents=[[word.lower() for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]\n",
    "\n",
    "    elif minimum_length>1:\n",
    "        normalized_sents= [[word for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]        \n",
    "        \n",
    "    return normalized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and pre-prcoessing\n",
    "\n",
    "with open(DATASEST_FILENAME, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "\n",
    "# merge all pages of each article\n",
    "articles = ['\\n'.join(article_pages_list) for article_pages_list in all_text]\n",
    "\n",
    "# merge all articles\n",
    "text = '\\n'.join(articles)\n",
    "text = sanitize_text(text)\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "tokenized_sentences = [[word for word in nltk.tokenize.word_tokenize(sent) if\n",
    "                        len(word)<30 and word != '...'] for sent in sentences]\n",
    "tokenized_sentences = filter(lambda x: len(x) > 6, tokenized_sentences)\n",
    "normalized_sentences = normalize_sentence(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet util functions\n",
    "\n",
    "def categories_from_hypernyms(termlist):\n",
    "    hypterms = []\n",
    "    hypterms_dict = defaultdict()\n",
    "    for term in termlist:                  # for each term\n",
    "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
    "        for syn in s:                      # for each lemma synset\n",
    "            for hyp in syn.hypernyms():    # It has a list of hypernyms\n",
    "                hypterms.append(hyp.name())      # Extract the hypernym name and add to list\n",
    "                if hyp.name() not in hypterms_dict:\n",
    "                    hypterms_dict[hyp.name()] = list()\n",
    "                hypterms_dict[hyp.name()].append(term)  # Extract examples and add them to dict\n",
    "                \n",
    "    hypfd = nltk.FreqDist(hypterms)             # After going through all the nouns, print out the hypernyms \n",
    "    for (name, count) in hypfd.most_common(25):  # that have accumulated the most counts (have seen the most descendents)\n",
    "        print( name, '({0})'.format(count))\n",
    "        print ('\\t', ', '.join(set(hypterms_dict[name])))  # show the children found for each hypernym\n",
    "        print ()\n",
    "        \n",
    "def freq_normed_unigrams(sents):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    tagged_POS_sents = [nltk.pos_tag(sent) for sent in sents]\n",
    "    \n",
    "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           for word in sent \n",
    "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           and word[0] not in punctuation\n",
    "                           and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           and word[1].startswith('N')]\n",
    "\n",
    "    top_normed_unigrams = [word for (word, count) in nltk.FreqDist(normed_tagged_words).most_common(40)]\n",
    "    return top_normed_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person.n.01 (4)\n",
      "\t user, case\n",
      "\n",
      "activity.n.01 (4)\n",
      "\t support, process, use\n",
      "\n",
      "body_part.n.01 (3)\n",
      "\t process, feature, system\n",
      "\n",
      "collection.n.01 (3)\n",
      "\t class, data, information\n",
      "\n",
      "cognition.n.01 (3)\n",
      "\t process, information\n",
      "\n",
      "concept.n.01 (3)\n",
      "\t section, rule\n",
      "\n",
      "direction.n.06 (3)\n",
      "\t rule\n",
      "\n",
      "hypothesis.n.02 (2)\n",
      "\t model, framework\n",
      "\n",
      "assistant.n.01 (2)\n",
      "\t model\n",
      "\n",
      "ideal.n.01 (2)\n",
      "\t model, example\n",
      "\n",
      "representation.n.01 (2)\n",
      "\t model, example\n",
      "\n",
      "method.n.01 (2)\n",
      "\t technique, system\n",
      "\n",
      "change.n.01 (2)\n",
      "\t transformation, development\n",
      "\n",
      "change_of_integrity.n.01 (2)\n",
      "\t transformation\n",
      "\n",
      "information.n.02 (2)\n",
      "\t example, data\n",
      "\n",
      "part.n.02 (2)\n",
      "\t element, section\n",
      "\n",
      "substance.n.01 (2)\n",
      "\t element\n",
      "\n",
      "use.n.01 (2)\n",
      "\t application, development\n",
      "\n",
      "part.n.01 (2)\n",
      "\t item\n",
      "\n",
      "fact.n.01 (2)\n",
      "\t item, case\n",
      "\n",
      "supporting_structure.n.01 (2)\n",
      "\t support, framework\n",
      "\n",
      "happening.n.01 (2)\n",
      "\t example, case\n",
      "\n",
      "gathering.n.01 (2)\n",
      "\t class\n",
      "\n",
      "time_period.n.01 (2)\n",
      "\t time\n",
      "\n",
      "resource.n.01 (2)\n",
      "\t support\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate different statastics\n",
    "\n",
    "# frequency statastics\n",
    "mp_freqdist = FreqDist(itertools.chain(*normalized_sentences))\n",
    "top20words=mp_freqdist.most_common(20)\n",
    "print ('%-16s' % 'word', '%-16s' % 'Frequency','%-16s' %  '% of the total')\n",
    "for topword in top20words:\n",
    "    percent=(topword[1]/len(normalized_sentences))*100\n",
    "    print ('%-16s' % topword[0], '%-16s' % topword[1],'%-16s' %  percent)\n",
    "\n",
    "print('\\n', 60*'-', '\\n')\n",
    "\n",
    "# word and snetence averages\n",
    "sentence_words = list(itertools.chain(*normalized_sentences))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in normalized_sentences])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])\n",
    "\n",
    "# WordNet\n",
    "top_words = freq_normed_unigrams(normalized_sentences)\n",
    "categories_from_hypernyms(top_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "312335cc34a7bdacf23af8f7d78a3e1ad583c5700a5052666157e9c43969dbf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
